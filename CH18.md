#CH18 Learning from examples
* Why do we want the agent to learn
	* Cannot anticipate all possible situations
	* Cannot anticipate all changes overtime
	* Human may not know how to program the solution themselves

## Forms of Learning

### Component to improve
* mapping condition -> action
* means to infer relevant properties
* info about how the world evolves, results of actions
* utility -> desirability of world states
* action-value -> desirability of actions
* goals (utitliy maximization)

### Prior knowledge & representation of data and component

* Propositional and first-order logical sentence
* Baysian networks
* Factored representation
	* vector of attribute values

### Feedback

* unsupervised learning
	* no explicit feedback
* reinforcement learning
	* rewards or punishments
* supervised learning
	* example input-output pairs
* semi-supervised learning
	* a few labeled examples
	* noise and lack of labels

## Supervised learnin

### Hypothesis
* **Training set**
    * example input-output pairs
    * (x<sub>1</sub>, y<sub>1</sub>) ... (x<sub>N</sub>, y<sub>N</sub>)
        * y = f(x)
    * Discover a function h (**hypothesis**) to approximate f
* **Test set**
	* distinct from  training set
* **Outputs**
    * **Classification**
        * Output y is a **finite set** of values
    * **Regression**
        * y is a number
* Choose among hypothesis
	* **hypothesis space**
		* space of possible h
	* **consistency**
		* hypothesis agrees with all the data
	* Always prefer the **simplest** hypothesis
		* Ockham's razor
    * tradeoff 1
    	* **complexity v.s. generalization**
    	* complex hypotheses fit the training data well
    	* simple hypotheses generalize better
    * realizable
    	* The hypothesis space contains the true function
    * How to choose using baye's rule
    	* h* = argmax_{h∈H} P(h|data)
    	* h* = argmax_{h∈H} P(data|h) P(h) (Baye's rule)
    * tradeoff 2
    	* **expresiveness v.s. complexity**
    	* **complexity** to find a good hypothesis within an **expressive** hypothesis space

## Learning decision trees

### Decision tree representation

* decision tree
	* a function
	* **input**: **vector** of attribute values
	* **output**: decision -- a value
* Boolean classification
	* Only two possible outputs
* How it works
	* Perform a sequence of tests
	* **internal node**: **test** on an attribute
	* **branch**: labeled with **possible values** of the tested attribute
	* **leaves**: **goal** attribute/predicate

### Expressiveness of decision trees

* Boolean decision tree is logically equivalent to the **assertion**
	* Goal <=> (Path1 ∪ Path2 ... )
* Some functions cannot be represented concisely
* complexity
	* n attributes
	* 2<sup>n</sup> rows in truth tables
	* 2<sup>2n</sup> functions in the hypothesis space

### Inducing decision trees from examples

* **Search** for a good **approximation** in this space efficiently
	* Use heuristics
	* Find a small (but not smallest) consistent tree
* **Decision tree learning** algorithm
	* Greedy & devide, conquer
    * Always test the **most important attribute first**
		* less possibile value, more important
    * After the split, each outcome is a new **subproblem**, with fewer examples and one less attribute

### Choosing attribute tests

### Generalization and overfitting

### Broadening the applicability of decision trees

